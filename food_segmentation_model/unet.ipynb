{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sm.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","executionInfo":{"elapsed":24632,"status":"ok","timestamp":1578588291275,"user":{"displayName":"Tsan-Chen Li","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA3tR5FDOUXJHCtYPt3zGN0e8n_UucWpIuAFX3_=s64","userId":"07716547533668306352"},"user_tz":300},"id":"ev0WaISiiyUl","outputId":"282ba34f-7540-4df0-ae2d-18b5f899077a"},"outputs":[],"source":["!pip install gpustat\n","!pip install --upgrade keras\n","!pip install --upgrade keras_preprocessing\n","!pip install segmentation_models\n","!pip install imgaug\n","!gpustat"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"colab_type":"code","executionInfo":{"elapsed":23919,"status":"ok","timestamp":1578588318433,"user":{"displayName":"Tsan-Chen Li","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA3tR5FDOUXJHCtYPt3zGN0e8n_UucWpIuAFX3_=s64","userId":"07716547533668306352"},"user_tz":300},"id":"fL4mjlczi8TO","outputId":"f011dd27-ea68-4cab-f3cc-120f8b3d72f8"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import keras\n","import segmentation_models as sm\n","import imgaug as ia\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"5kWObyuYjwi0"},"outputs":[],"source":["data_dir = '/content/gdrive/My Drive/data_source/food_segmentation/unet/training_data/train/'\n","label_dir = '/content/gdrive/My Drive/data_source/food_segmentation/unet/training_label/train/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85},"colab_type":"code","executionInfo":{"elapsed":1890,"status":"ok","timestamp":1578588512373,"user":{"displayName":"Tsan-Chen Li","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA3tR5FDOUXJHCtYPt3zGN0e8n_UucWpIuAFX3_=s64","userId":"07716547533668306352"},"user_tz":300},"id":"PrtMCMBnjxOF","outputId":"820a80df-81aa-475a-f626-147ecdc29999"},"outputs":[],"source":["BATCH_SIZE = 4\n","GENERATE_SEED = np.random.randint(0, 1024)\n","VALIDATION_SPLIT = 0.2\n","\n","maskgen_args = dict(\n","    rotation_range=180,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    vertical_flip=True,\n","    validation_split=VALIDATION_SPLIT\n",")\n","\n","def image_preprocessing(images):\n","    sometimes = lambda x: ia.augmenters.Sometimes(0.5, x)\n","    augmentation_sequence = ia.augmenters.Sequential([\n","        sometimes(ia.augmenters.AdditiveGaussianNoise(loc=(0.0, 0.1))),\n","        sometimes(ia.augmenters.Add(-0.1, 0.1))\n","    ])\n","    return augmentation_sequence(images=images)\n","\n","\n","imagegen_args = dict(\n","    samplewise_center=True,\n","    samplewise_std_normalization=True,\n","    preprocessing_function=image_preprocessing,\n","    **maskgen_args\n",")\n","\n","image_datagen = keras.preprocessing.image.ImageDataGenerator(**imagegen_args)\n","mask_datagen = keras.preprocessing.image.ImageDataGenerator(**maskgen_args)\n","\n","training_data_generator = zip(\n","    image_datagen.flow_from_directory(\n","        data_dir,\n","        target_size=(512, 512),\n","        color_mode='rgb',\n","        batch_size=BATCH_SIZE,\n","        class_mode=None,\n","        save_format='jpeg',\n","        seed=GENERATE_SEED,\n","        subset='training'\n","    ),\n","    mask_datagen.flow_from_directory(\n","        label_dir,\n","        target_size=(512, 512),\n","        color_mode='grayscale',\n","        batch_size=BATCH_SIZE,\n","        class_mode=None,\n","        save_format='png',\n","        seed=GENERATE_SEED,\n","        subset='training'\n","    )\n",")\n","\n","validation_data_generator = zip(\n","    image_datagen.flow_from_directory(\n","        data_dir,\n","        target_size=(512, 512),\n","        color_mode='rgb',\n","        batch_size=BATCH_SIZE,\n","        class_mode=None,\n","        save_format='jpeg',\n","        seed=GENERATE_SEED,\n","        subset='validation'\n","    ),\n","    mask_datagen.flow_from_directory(\n","        label_dir,\n","        target_size=(512, 512),\n","        color_mode='grayscale',\n","        batch_size=BATCH_SIZE,\n","        class_mode=None,\n","        save_format='png',\n","        seed=GENERATE_SEED,\n","        subset='validation'\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"yVGpJcSZBDRx"},"outputs":[],"source":["# Lovasz Softmax\n","# Reference: https://github.com/bermanmaxim/LovaszSoftmax\n","\n","def lovasz_grad(gt_sorted):\n","    \"\"\"\n","    Computes gradient of the Lovasz extension w.r.t sorted errors\n","    See Alg. 1 in paper\n","    \"\"\"\n","    gts = tf.reduce_sum(gt_sorted)\n","    intersection = gts - tf.math.cumsum(gt_sorted)\n","    union = gts + tf.math.cumsum(1. - gt_sorted)\n","    jaccard = 1. - intersection / union\n","    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n","    return jaccard\n","\n","\n","# --------------------------- BINARY LOSSES ---------------------------\n","def lovasz_hinge(labels, logits, per_image=True, ignore=None):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n","      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n","      per_image: compute the loss per image instead of per batch\n","      ignore: void class id\n","    \"\"\"\n","    if per_image:\n","        def treat_image(log_lab):\n","            log, lab = log_lab\n","            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n","            log, lab = flatten_binary_scores(log, lab, ignore)\n","            return lovasz_hinge_flat(log, lab)\n","        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n","        loss = tf.reduce_mean(losses)\n","    else:\n","        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n","    return loss\n","\n","\n","def lovasz_hinge_flat(logits, labels):\n","    \"\"\"\n","    Binary Lovasz hinge loss\n","      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n","      labels: [P] Tensor, binary ground truth labels (0 or 1)\n","      ignore: label to ignore\n","    \"\"\"\n","\n","    def compute_loss():\n","        labelsf = tf.cast(labels, logits.dtype)\n","        signs = 2. * labelsf - 1.\n","        errors = 1. - logits * tf.stop_gradient(signs)\n","        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n","        gt_sorted = tf.gather(labelsf, perm)\n","        grad = lovasz_grad(gt_sorted)\n","        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n","        return loss\n","\n","    # deal with the void prediction case (only void pixels)\n","    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n","                   lambda: tf.reduce_sum(logits) * 0.,\n","                   compute_loss,\n","                   strict=True,\n","                   name=\"loss\"\n","                   )\n","    return loss\n","\n","\n","def flatten_binary_scores(scores, labels, ignore=None):\n","    \"\"\"\n","    Flattens predictions in the batch (binary case)\n","    Remove labels equal to 'ignore'\n","    \"\"\"\n","    scores = tf.reshape(scores, (-1,))\n","    labels = tf.reshape(labels, (-1,))\n","    if ignore is None:\n","        return scores, labels\n","    valid = tf.not_equal(labels, ignore)\n","    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n","    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n","    return vscores, vlabels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":190},"colab_type":"code","executionInfo":{"elapsed":39978,"status":"ok","timestamp":1578588578093,"user":{"displayName":"Tsan-Chen Li","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA3tR5FDOUXJHCtYPt3zGN0e8n_UucWpIuAFX3_=s64","userId":"07716547533668306352"},"user_tz":300},"id":"2-aZAG5LkHY3","outputId":"1d0e8d78-259b-498d-dd17-8d7bd52385b1"},"outputs":[],"source":["unet_model = sm.Unet(backbone_name='inceptionresnetv2', encoder_freeze=True, classes=1, activation='linear')\n","unet_model.compile(\n","    optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n","    loss=lovasz_hinge,\n","    metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"colab_type":"code","executionInfo":{"elapsed":6241157,"status":"ok","timestamp":1578438705177,"user":{"displayName":"Tsan-Chen Li","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mA3tR5FDOUXJHCtYPt3zGN0e8n_UucWpIuAFX3_=s64","userId":"07716547533668306352"},"user_tz":300},"id":"cDbixsgwkG7P","outputId":"d788a7d4-2238-4b66-e543-f9f70aae6eae"},"outputs":[],"source":["unet_model.fit_generator(\n","    training_data_generator,\n","    epochs=6,\n","    steps_per_epoch=2048,\n","    callbacks=[keras.callbacks.ModelCheckpoint(\"/content/gdrive/My Drive/data_source/food_segmentation/sm_unet/model_checkpoint_{epoch:02d}.hdf5\")]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"xI3x55wmF7ty"},"outputs":[],"source":["unet_model.load_weights('/content/gdrive/My Drive/data_source/food_segmentation/sm_unet/6_epochs.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"colab_type":"code","id":"O_qVacOHbL86","outputId":"a08d203b-da03-40d6-b092-5d2e71ee8293"},"outputs":[],"source":["unet_model.fit_generator(\n","    training_data_generator,\n","    epochs=6,\n","    steps_per_epoch=2048,\n","    callbacks=[keras.callbacks.ModelCheckpoint(\"/content/gdrive/My Drive/data_source/food_segmentation/sm_unet/model_checkpoint_{epoch:02d}_2.hdf5\")]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"VFpBBghIHn7D"},"outputs":[],"source":[""]}]}